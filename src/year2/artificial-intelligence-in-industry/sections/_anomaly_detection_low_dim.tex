\chapter{Low dimensional anomaly detection: Taxi calls} \label{ch:ad_low}

\begin{description}
    \item[Anomaly] \marginnote{Anomaly}
        Event that deviates from the usual pattern.

    \item[Time series] \marginnote{Time series}
        Data with an ordering (e.g., chronological).
\end{description}



\section{Data}

The dataset is a time series and it is a \texttt{DataFrame} with the following fields:
\begin{descriptionlist}
    \item[\texttt{timestamp}] with a 30 minutes granularity.
    \item[\texttt{value}] number of calls.
\end{descriptionlist}

The label is a \texttt{Series} containing the timestamps of the anomalies.

An additional \texttt{DataFrame} contains information about the time window in which the anomalies happen:
\begin{descriptionlist}
    \item[\texttt{begin}] acceptable moment from which an anomaly can be detected.
    \item[\texttt{end}] acceptable moment from which there are no anomalies anymore.
\end{descriptionlist}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{./img/_ad_taxi_data.pdf}
    \caption{Plot of the time series, anomalies, and windows}
\end{figure}



\section{Approaches}


\subsection{Gaussian assumption}

Assuming that the data follows a Gaussian distribution, mean and variance can be used to determine anomalies through a threshold. $z$-score can also be used.


\subsection{Characterize data distribution}

Classify a data point as an anomaly if it is too unlikely.

\begin{description}
    \item[Problem formalization] 
        Given a random variable $X$ with values $x$ to represent the number of taxi calls, we want to find its probability density function (PDF) $f(x)$.

        An anomaly is determined whether:
        \[ f(x) \leq \varepsilon \]
        where $\varepsilon$ is a threshold.

        \begin{remark}
            A PDF can be reasonably used even though the dataset is discrete if its data points are sufficiently fine-grained.
        \end{remark}

        \begin{remark}
            It is handy to use negated log probabilities as:
            \begin{itemize}
                \item The logarithm adds numerical stability.
                \item The negation makes the probability an alarm signal, which is a more common measure.
            \end{itemize}
            Therefore, the detection condition becomes:
            \[ - \log f(x) \geq \varepsilon \]
        \end{remark}

    \item[Solution formalization]
        The problem can be tackled using a density estimation technique.
\end{description}


\subsubsection{Univariate kernel density estimation} \label{sec:ad_taxi_kde_uni}

\begin{description}
    \item[Kernel density estimation (KDE)] \marginnote{Kernel density estimation (KDE)}
        Based on the assumption that whether there is a data point, there are more around it. Therefore, each data point is the center of a density kernel.

        \begin{description}
            \item[Density kernel] 
                A kernel $K(x, h)$ is defined by:
                \begin{itemize}
                    \item The input variable $x$.
                    \item The bandwidth $h$.
                \end{itemize}
        
                \begin{description}
                    \item[Gaussian kernel]
                        Kernel defined as:
                        \[ K(x, h) \propto e^{-\frac{x^2}{2h^2}} \]
                        where:
                        \begin{itemize}
                            \item The mean is $0$.
                            \item $h$ is the standard deviation.
                        \end{itemize}
                        As the mean is $0$, an affine transformation can be used to center the kernel on a data point $\mu$ as $K(x - \mu, h)$.
                \end{description}
        \end{description}

        Given $m$ training data points $\bar{x}_i$, the density of any point $x$ can be computed as the kernel average:
        \[ f(x, \bar{x}, h) = \frac{1}{m} \sum_{i=0}^{m} K(x - \bar{x}_i, h) \]

        Therefore, the train data themselves are used as the parameters of the model while the bandwidth $h$ has to be estimated. 

    \item[Data split] 
        Time series are usually split chronologically:
        \begin{descriptionlist}
            \item[Train] Should ideally contain only data representing the normal pattern. A small amount of anomalies might be tolerated as they have low probabilities.
            \item[Validation] Used to find the threshold $\varepsilon$.
            \item[Test] Used to evaluate the model.
        \end{descriptionlist}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{./img/_ad_taxi_splits.pdf}
            \caption{Train, validation, and test splits}
        \end{figure}

    \item[Metrics] 
        It is not straightforward to define a metric for anomaly detection. A cost model to measure the benefits of a prediction is more suited. A simple cost model can be based on:
        \begin{descriptionlist}
            \item[True positives (\texttt{TP})] Windows for which at least an anomaly is detected;
            \item[False positives (\texttt{FP})] Detections that are not actually anomalies;
            \item[False negatives (\texttt{FN})] Undetected anomalies;
            \item[Advance (\texttt{adv})] Time between an anomaly and when it is first detected;
        \end{descriptionlist}
        and is computed as:
        \[ (c_\text{false} \cdot \texttt{FP}) + (c_\text{miss} \cdot \texttt{FN}) + (c_\text{late} \cdot \texttt{adv}_{\leq 0}) \]
        where $c_\text{false}$, $c_\text{miss}$, and $c_\text{late}$ are hyperparameters.

    \item[Bandwidth estimation]
        According to some statistical arguments, a rule-of-thumb to estimate $h$ in the univariate case is the following:
        \[ h = 0.9 \cdot \min\left\{ \hat{\sigma}, \frac{\texttt{IQR}}{1.34} \right\} \cdot m^{-\frac{1}{5}} \]
        where:
        \begin{itemize}
            \item $\texttt{IQR}$ is the inter-quartile range.
            \item $\hat{\sigma}$ is the standard deviation computed over the whole dataset.
        \end{itemize}

    \item[Threshold optimization]
        Using the train and validation set, it is possible to find the best threshold $\varepsilon$ that minimizes the cost model through linear search.

        \begin{remark}
            The train set can be used alongside the validation set to estimate $\varepsilon$ as this operation is not used to prevent overfitting.
        \end{remark}
\end{description}

\begin{remark}
    The evaluation data should be representative of the real world distribution. Therefore, in this case, to evaluate the model the whole dataset can be used.
\end{remark}

\begin{remark}
    KDE assumes that the Markov property holds. Therefore, each data point is considered independent to the others.
\end{remark}


\subsubsection{Multivariate kernel density estimation} \label{sec:ad_taxi_kde_multi}

\begin{remark}
    In this dataset, nearby points tend to have similar values.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\linewidth]{./img/_ad_taxi_subset.pdf}
        \caption{Subset of the dataset}
    \end{figure}
\end{remark}

\begin{description}
    \item[Autocorrelation plot]
        Plot to visualize the correlation between nearby points of a series.
        Given the original series, it is duplicated, shifted by a lag $l$, and the Pearson correlation coefficient is then computed between the two series. This operation is repeated over different values of $l$.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.75\linewidth]{./img/_ad_taxi_autocorrelation.pdf}
            \caption{
                \parbox[t]{0.6\linewidth}{
                    Autocorrelation plot of the subset of the dataset. There is strong correlation up to 4-5 lags.
                }
            } \label{fig:ad_taxi_autocorrelation}
        \end{figure}


    \item[Sliding window]
        Given a window size $w$ and a stride $s$, the dataset is split into sequences of $w$ continuous elements.

        \begin{remark}
            Incomplete sequences at the start and end of the dataset are ignored.
        \end{remark}

        \begin{remark}
            In \texttt{pandas}, the \texttt{rolling} method of \texttt{Dataframe} allows to create a slicing window iterator. This approach creates the windows row-wise and also considers incomplete windows.
            However, a usually more efficient approach is to construct the sequences column-wise by hand.
        \end{remark}

    \item[Multivariate KDE]
        Extension of KDE to vector variables.

    \item[Window size estimation]
        By analyzing the autocorrelation plot, an ideal window size can be picked as the lag with a low correlation (e.g., $10$ according to \Cref{fig:ad_taxi_autocorrelation}).

    \item[Bandwidth estimation]
        Differently from the univariate case, the bandwidth has to be estimated by maximizing the log-likelihood on the validation set. Given:
        \begin{itemize}
            \item The validation set $\tilde{x}$,
            \item The observations $x$,
            \item The bandwidth $h$,
            \item The density estimator $\hat{f}$
        \end{itemize}
        the likelihood is computed, by assuming independent observations, as:
        \[ L(h, x, \tilde{x}) = \prod_{i=1}^{m} \hat{f}(x_i, \tilde{x}_i, h) \]
        Maximum likelihood estimation is defined as:
        \[ \arg\max \mathbb{E}_{x \sim f(x), \tilde{x} \sim f(x)}[ L(h, x, \tilde{x}) ] \]
        where $f(x)$ is the true distribution.

        In practice, the expected value is sampled from multiple validation and training sets through cross-validation and grid-search.

        \begin{remark}
            As different folds for cross-validation must be tried, grid-search is an expensive operation.
        \end{remark}

    \item[Threshold optimization]
        The threshold can be determined as in \Cref{sec:ad_taxi_kde_uni}.
\end{description}


\subsubsection{Time-dependent estimator}

The approach taken in \Cref{sec:ad_taxi_kde_uni,sec:ad_taxi_kde_multi} is based on the complete timestamp of the dataset. Therefore, the same times on different days are treated differently. However, it can be seen that this time series is approximately periodic, making the approach used so far more complicated than necessary.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{./img/_ad_taxi_periodic.pdf}
    \includegraphics[width=0.8\linewidth]{./img/_ad_taxi_periodic_autocorrelation.pdf}
\end{figure}


\begin{description}
    \item[Time input]
        It is possible to take time into consideration by adding it as a parameter of the density function:
        \[ f(t, x) \]

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{./img/_ad_taxi_time_distribution.pdf}
            \caption{
                \parbox[t]{0.6\linewidth}{2D histogram of the distribution. Lighter colors indicate a higher frequency of occurrence.}
            }
        \end{figure}

        However, $t$ is a controlled variable and is completely predictable (i.e., if times are samples with different frequencies, they should not affect the overall likelihood). Therefore, a conditional density should be used:
        \[ f(x \mid t) = \frac{f(t, x)}{f(t)} \]
        where $f(t)$ can be easily computed using KDE on the time data.

        \begin{remark}
            For this dataset, the time distribution is uniform. Therefore, $f(t)$ is a constant and it is correct to use $f(t, x)$ as the estimator.
        \end{remark}

    \item[Bandwidth estimation]
        The bandwidth can be estimated as in \Cref{sec:ad_taxi_kde_multi}.

        \begin{remark}
            When using the \texttt{scikit-learn}, the dataset should be normalized as the implementation of KDE use the same bandwidth for all features.
        \end{remark}

    \item[Threshold optimization]
        The threshold can be determined as in \Cref{sec:ad_taxi_kde_uni}.
\end{description}


\subsubsection{Time-indexed model}

\begin{description}
    \item[Ensemble model] \marginnote{Ensemble model}
        Model defined as:
        \[ f_{g(t)}(x) \]
        where:
        \begin{itemize}
            \item $f_i$ are estimators, each working on subsets of the dataset and solving a smaller problem.
            \item $g(t)$ determines which particular $f_i$ should solve the input.
        \end{itemize}

    \item[Time-indexed model]
        Consider both time and sequence inputs by using an ensemble model. Each estimator is specialized on a single time value (i.e., an estimator for \texttt{00:00}, one for \texttt{00:30}, \dots ).

    \item[Bandwidth estimation]
        The bandwidth can be estimated as in \Cref{sec:ad_taxi_kde_multi}.

    \item[Threshold optimization]
        The threshold can be determined as in \Cref{sec:ad_taxi_kde_uni}.
\end{description}