\chapter{Knowledge injection}

\begin{remark}
    ML models exploit implicit knowledge from the data. Explicit knowledge (e.g., rules-of-thumb, known correlations and causal factors, laws of physics, \dots) is instead not used.
\end{remark}



\section{Approaches}


\subsection{Generative approach}

\begin{description}
    \item[Generative approach] \marginnote{Generative approach}
        Use symbolic knowledge to create training samples that are used to train a model.

        \begin{remark}
            This approach does not let the model exploit explicit knowledge and might be inefficient.
        \end{remark}
\end{description}



\subsection{Lagrangian approaches}

\begin{description}
    \item[Knowledge as constraint] 
        Use hard or soft constraints to inject knowledge.

        \begin{example}[RUL estimation]
            For RUL estimation, a simple explicit knowledge is that RUL decreases by 1 unit each time step. Formally, given two pairs of examples $(x_i, y_i)$ and $(x_j, y_j)$, it holds that:
            \[ y_i - y_j = j - i \quad \forall i,j = 1, \dots, m \text{ with } c_i = c_j \]
            where $c_k$ indicates the machine of the $k$-th sample.
            Given a model $f$, it can be constrained as follows:
            \[ f(x_i; \theta) - f(x_j; \theta) \approx j - i \]
            Its training problem becomes:
            \[ \arg\min_\theta \mathcal{L}(y, f(x; \theta)) \text{ subj. to } f(x_i; \theta) - f(x_j; \theta) \approx j - i \]
        \end{example}


    \item[Lagrangian approaches] \marginnote{Lagrangian approaches}
        Given:
        \begin{itemize}
            \item A training problem: $\arg\min_\theta \{ \mathcal{L}(\hat{y}) \mid \hat{y} = f(x; \theta) \}$,
            \item Some constraints defined as an inequality on a vector function $\vec{g}(\hat{y}) \leq 0$ with $\vec{g}(\hat{y}) = \{ g_k(\hat{y}) \}_{k=1}^{m}$.
        \end{itemize}

        \begin{description}
            \item[Naive formulation] 
                The constraints can be formulated as a loss penalty as follows:
                \[ \mathcal{L}(\theta, \vec{\lambda}) = \mathcal{L}(\hat{y}) + \vec{\lambda}^T \vec{g}(\hat{y}) \]
                where $\vec{\lambda}$ is a vector of multipliers (Lagrangian multipliers).

                \begin{remark}
                    $g_k(\hat{y}) = 0$ and $g_k(\hat{y}) \leq 0$ both satisfy the constraint. However, if $g_k(\hat{y})$ goes below $0$, it becomes an unwanted reward.

                    In classical Lagrangian theory, this is solved by changing the sign of $\lambda_k$. However, this works by assuming convexity and requires optimizing for $\vec{\lambda}$.
                \end{remark}

            \item[Clipped formulation] 
                The constraints can be formulated as a loss penalty as follows:
                \[ \mathcal{L}(\theta, \vec{\lambda}) = \mathcal{L}(\hat{y}) + \vec{\lambda}^T \max\{ 0, \vec{g}(\hat{y}) \} \]
                with $\vec{\lambda} \geq 0$.

                \begin{remark}
                    An equality constraint $g_k(\hat{y}) = 0$ can be formulated as:
                    \[ 
                        g_k(\hat{y}) \leq 0 \land -g_k(\hat{y}) \leq 0  
                    \]
                    In penalty terms, it becomes:
                    \[
                        \lambda_k' \max \{ 0, g_k(\hat{y}) \} + \lambda_k'' \max \{ 0, -g_k(\hat{y}) \} = \lambda_k | g_k(\hat{y}) |
                    \]
                    where $\lambda_k = \lambda_k' + \lambda_k''$.

                    An alternative formulation is also:
                    \[ \vec{\lambda}^T \vec{g}(\hat{y})^2 \]
                    which is due to normal distribution properties.
                \end{remark}
        \end{description}

        \begin{remark}
            Lagrangian approaches usually work with differentiable constraints, but it is not strictly required. However, differentiability is needed for training with gradient descent.
        \end{remark}

        \begin{description}
            \item[Multipliers calibration (maximal accuracy)]
                Consider constraints in a soft manner. The penalty can be considered as a regularizer and $\vec{\lambda}$ can be considered as a hyperparameter and optimized through hyperparameter tuning.

                \begin{remark}
                    This approach is viable when sufficient data is available.
                \end{remark}

            \item[Multipliers calibration (constraints satisfaction)]
                Consider constraints in a hard manner.

                \begin{description}
                    \item[Naive approach] 
                        Use a large $\vec{\lambda}$.
                        \begin{remark}
                            This approach leads to numerical instability and disproportional gradients.
                        \end{remark}

                \item[Dual ascent] \marginnote{Dual ascent}
                    \begin{remark}
                        The loss with the constraint penalty is differentiable in $\vec{\lambda}$:
                        \[ \nabla_\lambda \mathcal{L}(\theta, \vec{\lambda}) = \max \{ 0, \vec{g}(f(x, \theta)) \} \]
                        where:
                        \begin{itemize}
                            \item If the constraint is satisfied, the partial derivative is $0$.
                            \item If the constraint is violated, the partial derivative is equal to the violation.
                        \end{itemize}
                    \end{remark}

                    Alternate gradient descent and ascent. The method works as follows:
                    \begin{enumerate}
                        \item Initialize the multipliers $\vec{\lambda}^{(0)} = 0$.
                        \item Until a stop condition is met:
                        \begin{enumerate}
                            \item Obtain $\vec{\lambda}^{(k)}$ via gradient ascent with $\nabla_\vec{\lambda} \mathcal{L}(\theta^{(k-1)}, \vec{\lambda})$.
                            \item Obtain $\theta^{(k)}$ via gradient descent with $\nabla_\theta \mathcal{L}(\theta, \vec{\lambda}^{(k)})$.
                        \end{enumerate}
                    \end{enumerate}
                \end{description}
        \end{description}

        \begin{remark}
            Lagrangian approaches are enforced at training time. Constraints might be violated at test time.
        \end{remark}

        \begin{example}[RUL estimation]
            The constraint:
            \[ f(x_i; \theta) - f(x_j; \theta) \approx j - i \]
            can be formulated as the following penalty:
            \[ \lambda \sum_{\substack{i,j=1, \dots, m\\c_i = c_j}} (f(x_i; \theta) - f(x_j; \theta) - (j-i))^2 \]
            where $\lambda$ is the same for all pairs for simplicity.
            Moreover, to avoid redundances, it is reasonable to only consider consecutive time steps (denoted as $i \prec j$):
            \[ \lambda \sum_{\substack{i \prec j\\c_i = c_j}} (f(x_i; \theta) - f(x_j; \theta) - (j-i))^2 \]

            \indenttbox
            \begin{remark}
                With batches, consecutivity refers to the closest available time step  of the same machine in the same batch.

                It is also necessary to sample batches in such a way that at least two time steps of the same machine are considered (otherwise the gradient step would be wasted).
            \end{remark}

            \begin{figure}[H]
                \centering
                \begin{subfigure}{0.6\linewidth}
                    \centering
                    \includegraphics[width=\linewidth]{./img/_rul_pre_lagrangian.pdf}
                    \caption{Results without knowledge injection}
                \end{subfigure}
                \begin{subfigure}{0.6\linewidth}
                    \centering
                    \includegraphics[width=\linewidth]{./img/_rul_lagrangian.pdf}
                    \caption{Results with knowledge injection (note that the scales are off)}
                \end{subfigure}
            \end{figure}
        \end{example}
\end{description}


\subsection{Ordinary differential equations learning}

\begin{description}
    \item[Ordinary differential equation (ODE)] \marginnote{Ordinary differential equation (ODE)}
        Equation defined as:
        \[ \dot{y} = f(y, t) \]
        where $\dot{y}$ is the rate of change and $y$ is the state variable defined as a function of (usually) time $t$ (i.e., $y(t)$ is the state at time $t$).

        \begin{remark}
            This class of differential equations can be seen as a transition system with continuous steps.
        \end{remark}

        \begin{remark}
            ODEs are useful to represent physics knowledge.
        \end{remark}

    \item[Initial value problem] \marginnote{Initial value problem}
        Given an ODE $\dot{y} = f(y, t)$ and an initial state $y(0) = y_0$, determine how the states unfold (i.e., run a simulation).

        \begin{description}
            \item[Euler method] \marginnote{Euler method}
                Solve an initial value problem by discretizing time and computing the transition function at time $t$ as:
                \[ y_k = y_{k-1} + (t_k - t_{k-1}) f(y_{k-1}, t_{k-1}) \]
                It is assumed that the solution is piece-wise linear (i.e., linearity between two consecutive states).

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.7\linewidth]{./img/_rc_euler.pdf}
                \end{figure}

                \begin{remark}
                    Euler method does not perform well in terms of accuracy. There are variants with better performance.
                \end{remark}
        \end{description}

    \item[Learning ODE] \marginnote{Learning ODE}
        Estimate the parameters of an ODE from the data. The training problem is defined as:
        \[ \arg\min_\theta \{ \mathcal{L}(\hat{y}(t), y) \mid \dot{\hat{y}} = f(\hat{y}, t; \theta) \land \hat{y}(0) = y_0 \} \]

        A possible approach is to discretize time and optimize on the relaxed problem. The steps are:
        \begin{enumerate}
            \item Solve the initial value problem using a numerical method (e.g., Euler).
            \item Compute the loss $\mathcal{L}$ and optimize over $\theta$.
        \end{enumerate}

        \begin{remark}
            This path can be taken if the integration steps are differentiable. Euler method is differentiable.
        \end{remark}

        \begin{description}
            \item[Architecture]
                The method can be implemented in an RNN fashion with the Euler method encoded into a repeated layer. At each step, the networks takes as input the state and the time variable, and estimates a new state.

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.3\linewidth]{./img/rc_ode_learning.png}
                \end{figure}
        \end{description}

        \begin{remark}
            This approach is slow to converge and is not very accurate.
        \end{remark}

        \begin{example}
            Consider an RC circuit with a voltage source $V_S$, a capacitor $C$, and a resistor $R$.
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.23\linewidth]{./img/rc_circuit.png}
            \end{figure}
            Its dynamic behavior is described by the ODE:
            \[ \dot{V} = \frac{1}{\tau}(V_S - V) \]
            where $\tau = RC$.

            Assume that we have a dataset of the ground-truth voltage $y$ at each time step and we want to find the parameters $V_S$ and $\tau$ of the ODE. The problem is defined as:
            \[ \arg\min_{V_S, \tau} \mathcal{L}(\hat{y}(t), y) \text{ subj. to } \dot{\hat{y}} = \frac{1}{\tau} (V_S - \hat{y}) \land \hat{y}(0) = y_0 \]

            A layer of the neural network estimates $V_S$ and $\tau$. The time steps are passed through the network and all the predicted voltages are used to compute the loss.

            Note that $V_S$ and $\tau$ must be positive values. We can use the same trick as in \Cref{sec:arrivals_neuroprob} by using $\exp\log$ with a scaling factor to start with a reasonable guess.
        \end{example}

    \item[ODE learning improvements]
        \phantom{}
        \begin{description}
            \item[Training speed/Network depth]
                Given a sequence of training measurements $\{ y_k \}_{k=0}^n$, view them as a sequence of pairs $\{ (y_{k-1}, y_k) \}_{k=1}^n$. During training, each pair trains a distinct ODE, each with shared parameters. The overall problem becomes:
                \[
                    \begin{gathered}
                        \arg\min_\theta \sum_{k=1}^{n} \mathcal{L}(\hat{y}_k(t_k), y_k) \\
                        \begin{split}
                            \text{subject to }& \dot{\hat{y}}_k = f(\hat{y}_k, t; \theta) \qquad\forall k = 1, \dots, n \\
                            & \hat{y}_k(t_{k-1}) = y_{k-1} \qquad\forall k = 1, \dots, n
                        \end{split}
                    \end{gathered}
                \]

                \begin{remark}
                    This approach is only possible if full states are available and the loss is separable. Moreover, it is not strictly equivalent to the original problem as compound errors are disregarded.
                \end{remark}

                \begin{remark}
                    Training now requires shallower networks and mini-batches can be used.
                \end{remark}

            \item[Accuracy improvement]
                Accuracy issues are due to the fact that the Euler method intrinsically has low performance. As the problem consists of fitting a curve, the model might need to estimate wrong parameters to achieve a better result. 

                Therefore, if the objective is to fit a curve, this is not necessarily a problem. However, if the correct parameters are the goal, a more accurate integration method is needed.

                \begin{remark}
                    A straightforward approach to achieve better results is to increase the granularity of the steps of the Euler method (i.e., use more steps).
                \end{remark}
        \end{description}

    \item[Universal ODE (UDE)] \marginnote{Universal ODE (UDE)}
        Use black-box functions as the parameter of an ODE:
        \[ \dot{y} = f(y, t, U(y, t; \theta)) \]
        where $U$ is a universal approximator (e.g., a neural network).

        \begin{example}
            Consider the SIR (susceptible-infected-recovered) model for epidemic modeling:
            \[
                \begin{split}
                    \dot{S} &= -\beta \frac{1}{N} SI \\
                    \dot{I} &= +\beta \frac{1}{N} SI - \gamma I \\
                    \dot{R} &= +\gamma I
                \end{split}
            \]
            Non-pharmaceutical interventions (NPI) (e.g., masks) have an effect on $\beta$, making it time dependent:
            \[
                \begin{split}
                    \dot{S} &= -\beta(t) \frac{1}{N} SI \\
                    \dot{I} &= +\beta(t) \frac{1}{N} SI - \gamma I \\
                    \dot{R} &= +\gamma I
                \end{split}
            \]
            For this example, we assume $\beta(t)$ is modeled as:
            \[ \beta(t) = \beta_0 \prod_{i \in \mathcal{I}} e_i \]
            where $\beta_0$ is the baseline spread if nothing is done, $\mathcal{I}$ is the set of active NPIs, and $e_i$ is the effect of the $i$-th NPI.

            In practice, the time argument for $\beta$ is to retrieve the active NPIs at time $t$, therefore, the overall model can be formulated as:
            \[
                \begin{split}
                    \dot{S} &= -\beta(\texttt{NPI}(t)) \frac{1}{N} SI \\
                    \dot{I} &= +\beta(\texttt{NPI}(t)) \frac{1}{N} SI - \gamma I \\
                    \dot{R} &= +\gamma I
                \end{split}
            \]

            Architecturally, a neural network as in ODE can be used. The only addition is that it takes as input the active NPIs.
        \end{example}
\end{description}


\subsection{Physics informed neural network}

\begin{description}
    \item[Physics informed neural network (PINN)] \marginnote{Physics informed neural network (PINN)}
        Use a neural network to predict the state at time $t$:
        \[ \hat{y}(t; \theta) \approx y(t) \]

        \begin{remark}
            ODEs predict the derivative of the change of the state, while PINNs directly predict the next state. This allows for faster inference as integration (i.e., Euler method) is no longer needed.
        \end{remark}

        \begin{description}
            \item[Training]
                The training objective is similar to the one of UDE, with some changes to the constraints:
                \[
                    \begin{gathered}
                        \arg\min_\theta \mathcal{L}(\hat{y}(t, \theta), y) \\
                        \begin{split}
                            \text{subject to }& \dot{\hat{y}}(t; \theta) = f(\hat{y}(t; \theta), t) \\
                            & \hat{y}(t_0; \theta) = y_0
                        \end{split}
                    \end{gathered}
                \]
                where the first constraint imposes some physics knowledge and depends on the input (not the parameters).

                \begin{remark}
                    A PINN can be seen as neural network that learns ODE integration.
                \end{remark}

                In practice, the overall loss can be formulated as a Lagrangian:
                \[ 
                    \begin{split}
                        \mathcal{L}(y, \hat{y}, t, \theta) = &\mathcal{L}(\hat{y}(t; \theta), y) \\
                        &+ \lambda_{de}^T \Vert \dot{\hat{y}}(t; \theta) - f(\hat{y}(t; \theta), t) \Vert^2_2 \\
                        &+ \lambda_{bc}^T \Vert \dot{\hat{y}}(t; \theta) - y_0 \Vert^2_2 \\
                    \end{split}
                \]
                where $\lambda_{de}^T$ and $\lambda_{bc}^T$ are multipliers.

                \begin{remark}
                    If needed, the loss can be generalized further.
                \end{remark}

                \begin{remark}
                    If the physics ($f$) is known, the two constraints can be enforced without data points as only time steps (which can be discretized and decided a priori) are needed. Moreover, the derivative ($\dot{\hat{y}}(t; \theta)$) w.r.t. $t$ can be computed using automatic differentiation.

                    Therefore, during training, it is possible to rely on the two constraints more than the loss.
                \end{remark}

                \begin{remark}
                    Finding the weights $\lambda_{de}^T$ and $\lambda_{bc}^T$ can be tricky depending on the reliability and robustness of the physics model that is used. Cross-validation can be used if unsure and dual ascent can be used if the model is trusted.
                \end{remark}
        \end{description}
\end{description}