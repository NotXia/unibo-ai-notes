\chapter{Technical robustness and safety}


\begin{description}
    \item[AI act, article 15] \marginnote{AI act, article 15}
        Article related to accuracy, robustness, and cybersecurity. It states that high-risk AI systems should:
        \begin{itemize}
            \item Be benchmarked and evaluated adequately.
            \item Be resilient to errors.
            \item Have measures to prevent and respond to attacks.
        \end{itemize}

\end{description}

\begin{description}
    \item[Technical robustness and safety] \marginnote{Technical robustness and safety}
        AI systems should be secured to prevent unintentional harm and minimize the consequences of intentional harm. These requirements can be achieved by:
        \begin{itemize}
            \item Improving resilience to attacks.
            \item Introducing fallback plans.
            \item Improving general safety.
            \item Improving accuracy, reliability, and reproducibility.
        \end{itemize}

        \begin{remark}
            Robustness is required as the real-world distribution is usually different from the training one.
        \end{remark}
\end{description}

\begin{remark}[Reliability vs robustness vs resilience] \phantom{}
    \begin{descriptionlist}
        \item[Reliability] 
            Perform similarly on any test set from the same distribution.

            In practice, reliable design aims at obtaining a probability of failure below some threshold.

        \item[Robustness] 
            Perform reasonably well on test sets from a slightly different distribution.

            In practice, robust design aims at obtaining a model insensitive to small changes.

        \item[Resilience]
            Adapt to unexpected inputs from unknown distributions.
    \end{descriptionlist}
\end{remark}


\begin{description}
    \item[Robustness levels] \marginnote{Robustness levels}
        % Robustness can be achieved in terms of:
        % \begin{itemize}
        %     \item Generalization capabilities,
        %     \item Resistance to adversarial attacks,
        %     \item Resistance to data perturbation.
        % \end{itemize}
        Robustness can be ranked on different levels:
        \begin{descriptionlist}
            \item[Level 0] 
                No robustness measures or mitigation functionalities.

            \item[Level 1] 
                Generalization under distribution shift. It aims at mitigating data shifts and out-of-distribution data.
                
            \item[Level 2] 
                Robustness against a single risk.

            \item[Level 3] 
                Robustness against multiple risks.

            \item[Level 4] 
                Universal robustness against all known risks.

            \item[Level 5] 
                Level 4 system with human-aligned and augmented robustness.

        \end{descriptionlist}
\end{description}


\begin{description}
    \item[AI safety] \marginnote{AI safety}
        Build a system less vulnerable to adversarial attacks. This can be achieved by:
        \begin{itemize}
            \item Identifying anomalies.
            \item Defining safety objectives.
        \end{itemize}

    \item[Reproducibility] \marginnote{Reproducibility}
        Build a system that exhibits the same behavior under the same conditions.

        \begin{remark}[Repeatability vs replicability vs reproducibility]
            \phantom{}
            \begin{descriptionlist}
                \item[Repeatability] 
                    The same team can repeat the results under the same experimental setup.
                \item[Replicability] 
                    A different team can repeat the results under the same experimental setup.
                \item[Reproducibility] 
                    A different team can repeat the results with some tolerance under a different experimental setup.
            \end{descriptionlist}
        \end{remark}

\end{description}


\begin{description}
    \item[Robustness requirements] \marginnote{Robustness requirements}
        Two aspects have to be considered for robustness:
        \begin{descriptionlist}
            \item[Performance] 
                Capability of a model to perform a task reasonably well (humans can be used as baseline).

            \item[Vulnerability] 
                Resistance of the model to attacks. Possible sources of attack are: data poisoning, adversarial examples, flaws in the model.
        \end{descriptionlist}

    \item[Robustness approaches] \marginnote{Robustness approaches}
        Robustness can be imposed with different methods at different moments of the lifecycle of the system:
        \begin{itemize}
            \item Data sanitization,
            \item Robust learning,
            \item Extensive testing,
            \item Formal verification.
        \end{itemize}
\end{description}


\section{Robust learning}

\begin{description}
    \item[Robust learning]
        Learn a model that is general enough to handle slightly out-of-distribution data.
\end{description}

\begin{remark}
    It is impossible (and possibly unwanted) to have a system that models everything.

    \begin{theorem}[Fundamental theorem of machine learning]
        \[ \text{error rate} = \frac{\text{model complexity}}{\text{sample size}} \]
    \end{theorem}
    
    \begin{corollary}
        If the sample size is small, the model should be simple.
    \end{corollary}
\end{remark}

\begin{remark}[Uncertainty in AI]
    Knowledge in AI can be divided into:
    \begin{descriptionlist}
        \item[Known knowns] 
            Well-established and understood areas of research:
            \begin{itemize}
                \item Theorem proving.
                \item Planning in deterministic and fully-observed worlds.
                \item Games of perfect information.
            \end{itemize}

        \item[Known unknowns] 
            Areas whose understanding is incomplete:
            \begin{itemize}
                \item Probabilistic graphical models to represent and reason on uncertainty in complex systems.
                \item Probabilistic machine learning that is able to quantify uncertainty.
                \item Planning in Markov decision problems for decision-making under uncertainty.
                \item Computational game theory to analyze and solve games.
            \end{itemize}

        \item[Unknown unknowns] 
            Areas that we do not know are unknown. They are the natural step toward robust AI.
    \end{descriptionlist}
\end{remark}


\begin{remark}
    Robustness in biology is achieved by means of a diverse and redundant population of individuals.
\end{remark}


\subsection{Robustness to model errors}

\begin{description}
    \item[Robust optimization] \marginnote{Robust optimization}
        Handle uncertainty and variability through optimization methods:
        \begin{itemize}
            \item Assign ranges to parameters to account for uncertainty.
            \item Optimize in a max-min formulation aiming at maximizing the performance of the worst-case.
        \end{itemize}

        \begin{remark}
            There is a trade-off between optimality and robustness.
        \end{remark}

    \item[Model regularization] \marginnote{Model regularization}
        Add a penalty term to the training loss to encourage simple models.

        \begin{theorem}
            Regularization can be interpreted as robust optimization.
        \end{theorem}

    \item[Optimize risk-sensitive objectives] \marginnote{Optimize risk-sensitive objectives}
        Consider, when optimizing a reward, the variability and uncertainty associated to it (e.g., minimize variance of rewards).
    
    \item[Robust inference] \marginnote{Robust inference}
        Deal with uncertainty, noise, or variability at inference time.
\end{description}


\subsection{Robustness to unmodeled phenomena}

\begin{description}
    \item[Model expansion] \marginnote{Model expansion}
        Expand the models with a knowledge base.

        \begin{remark}
            New knowledge might contain errors or not improve the model at all.
        \end{remark}
    
    \item[Causal models] \marginnote{Causal models}
        Define causal relations.
    
    \item[Portfolio of models] \marginnote{Portfolio of models}
        Have multiple solvers available and use a selection method to choose the most suited in any situation.

        \begin{remark}
            Ideally, given an instance, there should be at least a solver that performs well on it.
        \end{remark}
    
    \item[Anomaly detection] \marginnote{Anomaly detection}
        Detect instances that deviate from the expected distribution.
\end{description}



\section{Data sanitization}

\begin{description}
    \item[Data sanitization] \marginnote{Data sanitization}
        Methods to ensure that data is deleted and unrecoverable.


    \item[NIST guidelines] \marginnote{NIST guidelines}
        Guidelines for media sanitization provided by the National Institute of Standards and Technology:
        \begin{enumerate}
            \item Determine the level of sanitization based on the sensitivity of the information.
            \item Choose a sanitization method such as:
            \begin{descriptionlist}
                \item[Clearing] Remove data so that it is not recoverable from software.
                \item[Purging] Physically remove the data on the media (e.g., degaussing a hard disk).
                \item[Destroying] Physically destroy the media.
            \end{descriptionlist}
            \item Document the sanitization process.
            \item Verify and validate the sanitization process.
            \item Create training and awareness programs on the importance of media sanitization.
        \end{enumerate}
\end{description}



\section{Extensive testing}

\begin{description}
    \item[Robustness testing] \marginnote{Robustness testing}
        Software test to evaluate the capability of a system to maintain its functionalities under unexpected scenarios.

        Key aspects to take into account are:
        \begin{itemize}
            \item Unexpected inputs,
            \item Edge cases,
            \item Stress testing and resource exhaustion,
            \item Fault tolerance,
            \item Error handling,
            \item Regression testing.
        \end{itemize}
\end{description}


\subsection{Model-based testing}

\begin{description}
    \item[Model-based testing] \marginnote{Model-based testing}
        Test the system on test cases generated based on a reference behavior model (i.e., expected output from a given input).

        \begin{remark}
            To exhaustively test all possible cases, the solution space is very large.
        \end{remark}

    \item[Search-based testing] \marginnote{Search-based testing}
        Use meta-heuristics to generate test cases.
\end{description}


\subsection{Results analysis testing}

\begin{description}
    \item[Passive testing] \marginnote{Passive testing}
        Add observation mechanisms to a system under test to collect and analyze execution traces. A possible approach works as follows:
        \begin{enumerate}
            \item Collect traces in case of failure.
            \item Define the properties the system should verify in case of failures.
            \item Check whether the execution trace satisfies the property.
        \end{enumerate}
\end{description}



\section{Formal verification}

\begin{description}
    \item[Formal specification] \marginnote{Formal specification}
        Unambiguous description of the system and its required properties.

    \item[Formal verification] \marginnote{Formal verification}
        Exhaustive comparison between the formal specification and the system.
\end{description}


\subsection{Theorem proving}

\begin{description}
    \item[Theorem proving] \marginnote{Theorem proving}
        Model the system as a set of logical formulae $\Gamma$ and the properties as theorems $\Psi$. Verification is done through logical reasoning:
        \[ \models (\Gamma \Rightarrow \Psi) \]
\end{description}

\begin{remark}
    Formalizing the system though logical formulae can be difficult.
\end{remark}


\subsection{Model checking}

\begin{description}
    \item[Model checking] \marginnote{Model checking}
        Model the system as a finite state machine $M$ and the properties as formal representations $\Psi$. Verification is done through logical reasoning:
        \[ M \models \Psi \]
\end{description}

\begin{remark}
    In practice, formal verification methods are difficult to scale due to the large state space.
\end{remark}



\section{Adversarial attacks}

\begin{description}
    \item[Adversarial attack] \marginnote{Adversarial attack}
        Techniques to intentionally manipulate the result of a machine learning model.

    \item[White-box attack] \marginnote{White-box attack}
        The adversary has complete knowledge of the attacked system.

    \item[Black-box attack] \marginnote{Black-box attack}
        The attacker can only interact with the system through input-output queries.
\end{description}


\subsection{Poisoning}

\begin{description}
    \item[Data poisoning] \marginnote{Data poisoning}
        Manipulate the training data.

    \item[Model poisoning] \marginnote{Model poisoning}
        Attack the model at training time (e.g., inject malicious gradients, modify loss, manipulate hyperparameters, \dots).

    \item[Algorithm poisoning] \marginnote{Algorithm poisoning}
        Modify the underlying algorithm, introduce backdoors, \dots.
\end{description}


\subsection{Model-based}

\begin{description}
    \item[Inversion attack] \marginnote{Inversion attack}
        Reconstruct information about the training data based on the model output.

    \item[Extraction attack] \marginnote{Extraction attack}
        Recover information of the underlying model (e.g., parameters, architecture, training data, \dots).
\end{description}


\subsection{Evasion}

\begin{description}
    \item[Score-based attack] \marginnote{Score-based attack}
        Manipulate the output logits to induce a misclassification.

    \item[Patch attack] \marginnote{Patch attack}
        Add imperceptible perturbations to the input to induce a misclassification.

    \item[Gradient attack] \marginnote{Gradient attack}
        Compute or estimate the gradient of the training loss function to determine a perturbation of the input to induce a misclassification.

    \item[Decision attack] \marginnote{Decision attack}
        Perturb the input to move across decision boundaries and induce a misclassification.

    \item[Adaptive attack] \marginnote{Adaptive attack}
        Dynamically adjust the attack strategy based on the model.
\end{description}