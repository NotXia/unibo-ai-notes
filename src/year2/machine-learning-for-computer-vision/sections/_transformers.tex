\chapter{Transformers in computer vision}



\section{Transformer}

\begin{description}
    \item[Transformer] \marginnote{Transformer}
        Neural architecture designed for NLP sequence-to-sequence tasks. It heavily relies on the attention mechanism.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\linewidth]{./img/transformer.png}
        \end{figure}

    \item[Autoregressive generation] \marginnote{Autoregressive generation}
        A transformer generates the output sequence progressively given the input sequence and the past outputted tokens. At the beginning, the first token provided as the past output is a special start-of-sequence token (\texttt{<SoS>}). Generation is terminated when a special end-of-sequence token (\texttt{<EoS>}) is generated.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.3\linewidth]{./img/_transformer_autoregressive.pdf}
            \caption{Example of autoregressive generation}
        \end{figure}
\end{description}


\subsection{Attention mechanism}


\begin{description}
    \item[Traditional attention] \marginnote{Traditional attention}
        Matrix computed by a neural network to weigh each token of a sequence against the tokens of another one.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.3\linewidth]{./img/traditional_attention.png}
            \caption{Attention weights for machine translation}
        \end{figure}

        \begin{remark}
            Before attention, for tasks such as machine translation, the whole input sequence was mapped into an embedding that is used to influence the generation of the output.
        \end{remark}

        \begin{remark}
            This is not the same attention of transformers as they do not directly compute attention weights between inputs and outputs.
        \end{remark}

    \item[Dot-product attention] \marginnote{Dot-product attention}
        Given $M$ input tokens $\matr{Y} \in \mathbb{R}^{M \times d_Y}$ and a vector $\vec{x}_1 \in \mathbb{R}^{d_Y}$, dot-product attention computes a linear combination of $\matr{Y}$ where each component is weighted based on a similarity score between $\matr{Y}$ and $\vec{x}_1$.

        This is done as follows:
        \begin{enumerate}
            \item Determine the similarity scores of the inputs as the dot-product between $\vec{x}_1$ and $\matr{Y}^T$:
                \[ \vec{x}_1 \matr{Y}^T \in \mathbb{R}^{M} \]
            \item Compute the attention weights by applying the \texttt{softmax} function on the similarity scores:
                \[ \texttt{softmax}(\vec{x}_1 \matr{Y}^T) \in \mathbb{R}^{M} \]
            \item Determine the output activation $\vec{a}_1$ as the dot-product between the attention weights and the input $\matr{Y}$:
                \[ \mathbb{R}^{d_Y} \ni \vec{a}_1 = \texttt{softmax}(\vec{x}_1 \matr{Y}^T) \matr{Y} \]
        \end{enumerate}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{./img/_dot_product_attention.pdf}
            % \caption{Steps of dot-product attention}
        \end{figure}

    \item[Scaled dot-product attention] \marginnote{Scaled dot-product attention}
        To add more flexibility, a linear transformation can be applied on the inputs $\matr{Y}$ and $\vec{x_1}$ to obtain:
        \begin{descriptionlist}
            \item[Keys] 
                With the projection $\matr{W}_K \in \mathbb{R}^{d_Y \times d_K}$ such that $\mathbb{R}^{M \times d_K} \ni \matr{K} = \matr{Y} \matr{W}_K$, where $d_K$ is the dimension of the keys.

            \item[Query] 
                With the projection $\matr{W}_Q \in \mathbb{R}^{d_X \times d_K}$ such that $\mathbb{R}^{d_K} \ni \vec{q}_1 = \matr{Y} \matr{W}_X$, where $d_X$ is the length of $\vec{x}_1$ that is no longer required to be $d_Y$ as there is a projection.

            \item[Values]
                With the projection $\matr{W}_V \in \mathbb{R}^{d_Y \times d_V}$ such that $\mathbb{R}^{M \times d_V} \ni \matr{V} = \matr{Y} \matr{W}_K$, where $d_V$ is the dimension of the values.
        \end{descriptionlist}

        The attention mechanism is then defined as:
        \[ \vec{a}_1 = \texttt{softmax}(\vec{q}_1 \matr{K}^T)\matr{V} \]

        To obtain smoother attention weights when working with high-dimensional activations (i.e., avoid a one-hot vector from \texttt{softmax}), a temperature of $\sqrt{d_K}$ is applied to the similarity scores:
        \[ \vec{a}_1 = \texttt{softmax}\left( \frac{\vec{q}_1 \matr{K}^T}{\sqrt{d_K}} \right)\matr{V} \]

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{./img/_scaled_dot_attention.pdf}
            % \caption{Steps of scaled dot-product attention}
        \end{figure}

        Finally, due to the linear projections, instead of a single vector there can be an arbitrary number $N$ of inputs $\matr{X} \in \mathbb{R}^{N \times d_X}$ to compute the queries $\mathbb{R}^{N \times d_K} \ni \matr{Q} = \matr{X} \matr{W}_Q$. This change affects the similarity scores $\matr{Q}\matr{K}^T \in \mathbb{R}^{N \times M}$ and the output activations $\matr{A} \in \mathbb{R}^{N \times d_V}$. 

        The overall attention mechanism can be defined as:
        \[ \matr{A} = \texttt{softmax}_\texttt{row-wise}\left( \frac{\matr{Q}\matr{K}^T}{\sqrt{d_K}} \right) \matr{V} \]

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{./img/_scaled_dot_attention_multi_q.pdf}
            % \caption{Steps of scaled dot-product attention with multidimensional queries}
        \end{figure}

    \item[Self-attention] \marginnote{Self-attention}
        Scaled dot-product attention mechanism where the inputs to compute keys, queries, and values are the same.

        Given an input $\matr{Y} \in \mathbb{R}^{N \times d_Y}$, the shape of each component is: $\matr{K} \in \mathbb{R}^{N \times d_K}$, $\matr{Q} \in \mathbb{R}^{N \times d_K}$, $\matr{V} \in \mathbb{R}^{N \times d_V}$, and $\matr{A} \in \mathbb{R}^{N \times d_V}$.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{./img/_self_attention.pdf}
            % \caption{Steps of self-attention}
        \end{figure}
\end{description}


\subsection{Embeddings}

\begin{description}
    \item[Embedding layer] \marginnote{Embedding layer}
        Converts input tokens into their corresponding learned embeddings of shape $d_Y$ (usually denoted as $d_\text{model}$).

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\linewidth]{./img/_transformer_embeddings.pdf}
        \end{figure}
\end{description}


\subsection{Encoder}

\begin{description}
    \item[Encoder components]
        A transformer encoder is composed of:

        \begin{description}
            \item[Multi-head self-attention (\texttt{MHSA})] \marginnote{Multi-head self-attention}
                Given an input $\matr{Y} \in \mathbb{R}^{M \times d_Y}$, a \texttt{MHSA} block parallelly passes it through $h$ different self-attention blocks to obtain the activations $\matr{A}^{(1)}, \dots, \matr{A}^{(h)}$. The output $\matr{A}$ of the block is obtained as a linear projection of the column-wise concatenation of the activations $\matr{A}^{(i)}$:
                \[ \mathbb{R}^{M \times d_Y} \ni \matr{A} = \left[ A^{(1)} \vert \dots \vert A^{(h)} \right] \matr{W}_O \]
                where $\matr{W}_O \in \mathbb{R}^{hd_V \times d_Y}$ is the projection matrix.

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.7\linewidth]{./img/_multi_head_attention.pdf}
                    \caption{\texttt{MHSA} with two heads}
                \end{figure}

                \begin{remark}
                    The idea of multiple attention heads is to allow the model to attend to information from different representation subspaces.
                \end{remark}

                \begin{remark}
                    Even though they can be freely set, the dimensions for queries, keys, and values of each attention head usually are $d_K = d_V = d_Y/h$.
                \end{remark}

            \item[Layer normalization (\texttt{LN})] \marginnote{Layer normalization}
                Normalize each input activation independently to have zero mean and unit variance, regardless of the other activations in the batch.

                Given $B$ activations $\vec{a}^{(i)} \in \mathbb{R}^{D}$, mean and variance of each activation $i=1, \dots, B$ are computed as:
                \[ \mu^{(i)} = \frac{1}{D} \sum_{j=1}^{D} \vec{a}^{(i)}_{j} \qquad v^{(i)} = \frac{1}{D} \sum_{j=1}^{D} \left( \vec{a}^{(i)}_j - \mu^{(i)} \right)^2 \]
                Each component $j$ of the normalized activation $\hat{\vec{a}}^{(i)}$ is computed as:
                \[ \hat{\vec{a}}^{(i)}_{j} = \frac{\vec{a}^{(i)}_j - \mu^{(i)}}{\sqrt{v^{(i)} + \varepsilon}} \]
                As in batch normalization, the actual output activation $\vec{s}^{(i)}$ of each input $\vec{a}^{(i)}$ is scaled and offset by learned values:
                \[ \vec{s}^{(i)}_j = \vec{\gamma}_j \hat{\vec{a}}^{(i)}_j + \vec{\beta}_j \]

                \begin{remark}
                    Differently from computer vision, in NLP the input is not always of the same length and padding is needed. Therefore, batch normalization do not always work well.
                \end{remark}

                \begin{remark}
                    Layer normalization is easier to distribute on multiple computation units and has the same behavior at both train and inference time.
                \end{remark}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.8\linewidth]{./img/norm_methods.png}
                    \caption{Affected axis of normalization methods}
                \end{figure}

            \item[Feed-forward network (\texttt{FFN})] \marginnote{Feed-forward network}
                MLP with one hidden layer applied to each token independently. ReLU or one of its variants are used as activation function:
                \[ \texttt{FFN}(\vec{x}) = \texttt{relu}(\vec{x}\matr{W}_1 + \vec{b}_1)\matr{W}_2 + \vec{b}_2 \]

                \begin{remark}
                    It can be implemented using two 1D convolutions with kernel size $1$.
                \end{remark}

            \item[Residual connection] \marginnote{Residual connection}
                Around the \texttt{MHSA} and \texttt{FFN} modules.
        \end{description}


    \item[Encoder stack] \marginnote{Encoder stack}
        Composed of $L$ encoder layers.

    \item[Encoder layer] \marginnote{Encoder layer}
        Layer to compute a higher level representation of each input token while maintaining the same length of $d_Y$. 
        
        Given the input tokens $\matr{H}^{(i)} = [ \vec{h}^{(i)}_1, \dots, \vec{h}^{(i)}_N ]$, depending on the position of layer normalization, an encoder layer computes the following:
        \begin{descriptionlist}
            \item[Post-norm transformer] Normalization is done after the residual connection:
                \[
                    \begin{split}
                        \bar{\vec{h}}^{(i)}_j &= \texttt{LN}\left( \vec{h}^{(i)}_j + \texttt{MHSA}_{\matr{H}^{(i)}}(\vec{h}^{(i)}_j) \right) \\
                        \vec{h}^{(i+1)}_j &= \texttt{LN}\left( \bar{\vec{h}}^{(i)}_j + \texttt{FNN}(\bar{\vec{h}}^{(i)}_j) \right) \\
                    \end{split}
                \]

                \begin{remark}
                    In post-norm transformers, residual connections are ``disrupted'' by layer normalization.
                \end{remark}

            \item[Pre-norm transformer] Normalization is done inside the residual connection:
                \[
                    \begin{split}
                        \bar{\vec{h}}^{(i)}_j &= \vec{h}^{(i)}_j + \texttt{MHSA}_{\matr{H}^{(i)}}\left( \texttt{LN}(\vec{h}^{(i)}_j) \right) \\
                        \vec{h}^{(i+1)}_j &= \bar{\vec{h}}^{(i)}_j + \texttt{FNN}\left( \texttt{LN}(\bar{\vec{h}}^{(i)}_j) \right) \\
                    \end{split}
                \]

                \begin{remark}
                    In practice, with pre-norm transformer training is more stable.
                \end{remark}
        \end{descriptionlist}

        \begin{figure}[H]
            \centering
            \begin{subfigure}{0.40\linewidth}
                \centering
                \includegraphics[width=0.8\linewidth]{./img/_post_norm_encoder.pdf}
                \caption{Encoder in post-norm transformer}
            \end{subfigure}
            \begin{subfigure}{0.40\linewidth}
                \centering
                \includegraphics[width=0.8\linewidth]{./img/_pre_norm_encoder.pdf}
                \caption{Encoder in pre-norm transformer}
            \end{subfigure}
        \end{figure}

        \begin{remark}
            Of all the components in an encoder, attention heads are the only one that allow interaction between tokens.
        \end{remark}
\end{description}


\subsection{Decoder}

\begin{description}
    \item[Decoder stack] \marginnote{Decoder stack}
        Composed of $L$ decoder layers.

    \item[Decoder layer] \marginnote{Decoder layer}
        Layer to autoregressively generate tokens.

        Its main components are:
        \begin{descriptionlist}
            \item[Multi-head self-attention]
                Processes the input tokens.

            \item[Encoder-decoder multi-head attention/Cross-attention] \marginnote{Cross-attention}
                Uses as query the output of the previous \texttt{MHSA} layer, and as keys and values the output of the encoder stack. In other words, it allows the tokens passed through the decoder to attend the input sequence.

                \begin{remark}
                    The output of cross-attention can be seen as an additive delta to improve the activations $\vec{z}^{(i)}_j$ obtained from the first MHSA layer.
                \end{remark}

                \begin{remark}
                    As queries are independent to each other, and keys and values are constants coming from the encoder, cross-attention works in a token-wise fashion.
                \end{remark}

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.75\linewidth]{./img/_cross_attention.pdf}
                    \caption{Cross-attention data flow}
                \end{figure}

            \item[Feed-forward network]
                MLP applied after cross-attention.
        \end{descriptionlist}

        \begin{remark}
            As for the encoder, there is a post-norm and pre-norm formulation.
        \end{remark}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.5\linewidth]{./img/_transformer_decoder.pdf}
            \caption{Decoder in post-norm transformer}
        \end{figure}


    \item[Parallel training]
        When training, as the ground truth is known, it is possible to train all decoder outputs in a single pass. Given a target sequence \texttt{[$\texttt{<SoS>}, t_1, \dots, t_n, \texttt{<EoS>}$]}, it is processed by the decoder in the following way:
        \begin{itemize}
            \item The input is \texttt{[$\texttt{<SoS>}, t_1, \dots, t_n$]} (i.e., without end-of-sequence token).
            \item The expected output \texttt{[$t_1, \dots, t_n, \texttt{<EoS>}$]} (i.e., without start-of-sequence token).
        \end{itemize}
        In other words, with a single pass, it is expected that each input token generates the correct output token.

        \begin{remark}
            Without changes to the self-attention layer, a token at position $i$ in the input is able to attend to future tokens at position $\geq i+1$. This causes a data leak as, during inference, autoregressive generation do not have access to future tokens.
        \end{remark}

        \begin{description}
            \item[Masked self-attention] \marginnote{Masked self-attention}
                Modification to self-attention to prevent tokens to attend at future positions (i.e., at their right). This can be done by either setting the similarity scores with future tokens to $-\infty$ or directly setting the corresponding attention weights to $0$ (i.e., make the attention weights a triangular matrix).

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.8\linewidth]{./img/_masked_self_attention.pdf}
                \end{figure}
        \end{description}
\end{description}


\subsection{Positional encoding}

\begin{remark}[Self-attention equivariance to permutation]
    By permuting the input sequence of a self-attention layer, the corresponding outputs will be the same as if it were the original sequence, but it is affected by the same permutation. Therefore, self-attention alone does not have information on the ordering of the tokens.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{./img/_self_attention_permutation.pdf}
    \end{figure}
\end{remark}

\begin{description}
    \item[Positional encoding] \marginnote{Positional encoding}
        Vector of shape $d_Y$ added to the embeddings to encode positional information. Positional encoding can be:
        \begin{descriptionlist}
            \item[Fixed] 
                The vector associated to each position is fixed and known before training.
                \begin{example}
                    The original transformer paper proposed the following encoding:
                    \[
                        \texttt{pe}_{\texttt{pos}, 2i} = \sin \left( \frac{\texttt{pos}}{10000^{2i/d_Y}} \right)
                        \qquad
                        \texttt{pe}_{\texttt{pos}, 2i+1} = \cos \left( \frac{\texttt{pos}}{10000^{2i/d_Y}} \right)
                    \]
                    where $\texttt{pos}$ indicates the position of the token and $i$ is the dimension of the position encoding vector (i.e., even indexes use $\sin$ and odd indexes use $\cos$).
                \end{example}

            \item[Learned] 
                The vector for position encoding is learned alongside the other parameters.
        \end{descriptionlist}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.65\linewidth]{./img/_transformer_position_encoding.pdf}
        \end{figure}
\end{description}


\begin{remark}[Transformer vs recurrent neural networks]
    Given a sequence of $n$ tokens with $d$-dimensional embeddings, self-attention and RNN can be compared as follows:
    \begin{itemize}
        \item The computational complexity of self-attention is $O(n^2 \cdot d)$ whereas for RNN is $O(n \cdot d^2)$. Depending on the task, $n$ might be a big value.
        \item The number of sequential operations for training is $O(1)$ for self-attention (parallel training) and $O(n)$ for RNN (not parallelizable).
        \item The maximum path length (i.e., maximum number of operations before a token can attend to all the others) is $O(1)$ for self-attention (through the multi-head self-attention layer) and $O(n)$ for RNN (it needs to process each token individually while maintaining a memory).
    \end{itemize}
\end{remark}



\section{Vision transformer}

\begin{remark}
    Using single pixels as tokens is unfeasible due to the complexity scaling of transformers as an $H \times W$ image results in an attention matrix of $(HW)^2$ entries.

    \indenttbox
    \begin{example}
        Consider an ImageNet image with shape $224 \times 224$. The attention weights will have $(224^2)^2 = 2.5 \text{ bln}$ entries which would require $5 \text{ GB}$ to store them in half-precision. A classic $12$ layers with $8$ heads transformer would require $483 \text{ GB}$ of memory to only store all attention matrices.
    \end{example}
\end{remark}

\begin{remark}
    Compared to text, image pixels are more redundant and less semantically rich. Therefore, processing all of them together is not strictly necessary.
\end{remark}

\begin{description}
    \item[Patch] \marginnote{Patch}
        Given an image of size $C \times H \times W$, it is divided into patches of size $P \times P$ along the spatial dimension. Each patch is converted into a $Y_D$-dimensional embedding for a transformer as follows:
        \begin{enumerate}
            \item Flatten the patch into a $P^2C$ vector.
            \item Linearly transform it through a learned projection matrix $W_E \in \mathbb{R}^{P^2C \times Y_D}$.
            \item Add positional information.
        \end{enumerate}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{./img/_vit_patch.pdf}
        \end{figure}

    \item[Vision transformer (ViT)] \marginnote{Vision transformer (ViT)}
        Transformer encoder that processes embedded patches. A special classification token (\texttt{[CLS]}, as in BERT) is appended at the beginning of the sequence to encode the image representation and its embedding is passed through a traditional classifier to obtain the logits.

        \begin{remark}
            The (pre-norm) transformer encoder used in vision is the same one as in NLP.
        \end{remark}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.55\linewidth]{./img/_vision_transformer.pdf}
        \end{figure}

        \begin{remark}
            Differently from convolutional neural networks where convolutions are the major source of FLOPs, in ViT the number of FLOPs heavily depends on the length of the input sequence due to the quadratic complexity of the attention mechanism.
        \end{remark}

    \item[ViT variants]
        The main size-wise variants of ViT are the following:
        \begin{center}
            \small
            \begin{tabular}{cccccc}
                \toprule
                \textbf{Model} & \textbf{Layers} & \textbf{Heads} & \textbf{Hidden size} & \textbf{MLP size} & \textbf{Parameters} \\
                \midrule
                ViT-base    & 12 & 12 & 768 & 3072 & 86 M \\
                ViT-large   & 24 & 16 & 1024 & 4096 & 307 M \\
                ViT-huge    & 32 & 16 & 1280 & 5120 & 632 M \\
                \bottomrule
            \end{tabular}
        \end{center}
        Note that, by convention, the MLP size is four times the hidden size.

        Moreover, ViT models can also vary depending on the size of the input patch.

        The overall notation to denote size and patch is: ViT-\texttt{<size>}/\texttt{<patch size>}.

    \item[Results]
        The main experimental observations and results using vision transformer are the following:
        \begin{itemize}
            \item The first embedding projection $W_E$ for RGB images shows a similar behavior to convolutions as they tend to recognize edges and color variations.
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.45\linewidth]{./img/_vit_projection_rgb.pdf}
                \caption{
                    \parbox[t]{0.7\linewidth}{
                        Visualization of the columns of the patches linear projection matrix $W_E$. Each column has shape $3P^2$ and can be reshaped to be a $3 \times P \times P$ image.
                    }
                }
            \end{figure}

            \item The learned positional embeddings are able to encode information about the row and column positioning of the patches.
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.33\linewidth]{./img/_vit_embedding_similarity.pdf}
                \caption{
                    \parbox[t]{0.7\linewidth}{
                        Cosine similarity of the positional encoding of each patch compared to all the others
                    }
                }
            \end{figure}

            \item Attention heads at the lower layers attend at both positions around the patch and far from them. Higher layers, as with convolutions, attend to distant patches.
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.33\linewidth]{./img/_vit_head_distance.pdf}
                \caption{
                    Mean attention distance of the heads of ViT-large/16
                }
            \end{figure}

            \item On ImageNet top-1 accuracy, ViT outperforms a large ResNet only when pre-trained on a large dataset.
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.4\linewidth]{./img/_vit_results.pdf}
                \caption{
                    \parbox[t]{0.7\linewidth}{
                        ImageNet top-1 accuracy with different pre-training datasets. BiT represents ResNet (two variants).
                    }
                }
            \end{figure}
        \end{itemize}

        \begin{remark}
            Comparison between convolutional neural networks and vision transformer is not straightforward.
        \end{remark}

        \begin{remark}
            On an execution efficiency point-of-view, the currently more common inference hardware is more optimized for convolutions.
        \end{remark}
\end{description}