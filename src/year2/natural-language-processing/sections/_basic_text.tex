\chapter{Basic text processing}


\begin{description}
    \item[Text normalization]
        Operations such as:
        \begin{description}
            \item[Tokenization] \marginnote{Tokenization}
            Split a sentence in tokens.

            \begin{remark}
                Depending on the approach, a token is not always a word.
            \end{remark}

            \item[Lemmatization/stemming] \marginnote{Lemmatization/stemming}
                Convert words to their canonical form.

                \begin{example}
                    $\{ \texttt{sang}, \texttt{sung}, \texttt{sings} \} \mapsto \texttt{sing}$
                \end{example}

            \item[Sentence segmentation] \marginnote{Sentence segmentation}
                Split a text in sentences.

                \begin{remark}
                    A period does not always signal the end of a sentence.
                \end{remark}
        \end{description}
\end{description}


\section{Regular expressions}

\begin{description}
    \item[Regular expression (regex)] \marginnote{Regular expression (regex)}
        Formal language to describe string patterns.
\end{description}


\subsection{Basic operators}

\begin{description}
    \item[Disjunction (brackets)]
        Match a single character between square brackets \texttt{[]}.

        \begin{example}
            \texttt{/[wW]oodchuck/} matches \texttt{Woodchuck} and \texttt{woodchuck}.
        \end{example}

    \item[Range]
        Match a single character from a range of characters or digits.

        \begin{example} \phantom{}\\[-1.5em]
            \begin{itemize}
                \item \texttt{/[A-Z]/} matches a single upper case letter.
                \item \texttt{/[a-z]/} matches a single lower case letter.
                \item \texttt{/[0-9]/} matches a single digit.
            \end{itemize}
        \end{example}

    \item[Negation]
        Match the negation of a pattern.

        \begin{example}
            \texttt{/[\textasciicircum A-Z]/} matches a single character that is not an upper case letter.
        \end{example}

    \item[Disjunction (pipe)]
        Disjunction of regular expressions separated by \texttt{|}.

        \begin{example}
            \texttt{/groundhog|woodchuck/} matches \texttt{groundhog} and \texttt{woodchuck}.
        \end{example}


    \item[Wildcards] \phantom{}
        \begin{description}
            \item[Optional]
                A character followed by \texttt{?} can be matched optionally.

                \begin{example}
                    \texttt{/woodchucks?/} matches \texttt{woodchuck} and \texttt{woodchucks}.
                \end{example}

            \item[Any]
                \texttt{.} matches any character.

            \item[Kleene \texttt{*}]
                A character followed by \texttt{*} can be matched zero or more times.

            \item[Kleene \texttt{+}]
                A character followed by \texttt{+} must be matched at least once.

            \item[Counting]
                A character followed by \texttt{\string{n,m\string}} must be matched from $n$ to $m$ times.

                \begin{example} \phantom{}\\[-1.5em]
                    \begin{itemize}
                        \item \texttt{\string{n\string}} matches exactly $n$ instances of the previous character.

                        \item \texttt{\string{n,m\string}} matches from $n$ to $m$ instances of the previous character.

                        \item \texttt{\string{n,\string}} matches at least $n$ instances of the previous character.
                        
                        \item \texttt{\string{,m\string}} matches at most $m$ instances of the previous character.
                    \end{itemize}
                \end{example}
        \end{description}

    \item[Anchors] \phantom{}
        \begin{description}
            \item[Start of line]
                \texttt{\textasciicircum} matches only at the start of line.

                \begin{example}
                    \texttt{/\textasciicircum a/} matches \texttt{\underline{a}} but not \texttt{ba}.
                \end{example}

            \item[End of line] 
                \texttt{\$} matches only at the end of line.

                \begin{example}
                    \texttt{/a\$/} matches \texttt{\underline{a}} but not \texttt{ab}.
                \end{example}

            \item[Word boundary]
                \texttt{\char`\\ b} matches a word boundary character.

            \item[Word non-boundary] 
                \texttt{\char`\\ B} matches a word non-boundary character.
        \end{description}

    \item[Aliases] \phantom{}
        \begin{itemize}
            \item \texttt{\char`\\ d} matches a single digit (same as \texttt{[0-9]}).
            
            \item \texttt{\char`\\ D} matches a single non-digit (same as \texttt{[\textasciicircum\char`\\ d]}).
            
            \item \texttt{\char`\\ w} matches a single alphanumeric or underscore character (same as \texttt{[a-zA-Z0-9\_]}).
            
            \item \texttt{\char`\\ w} matches a single non-alphanumeric and non-underscore character (same as \texttt{[\textasciicircum\char`\\ w]}).

            \item \texttt{\char`\\ s} matches a single whitespace (space or tab).
            
            \item \texttt{\char`\\ S} matches a single non-whitespace.
        \end{itemize}


    \item[Capture group]
        Operator to refer to previously matched substrings.

        \begin{example}
            In the regex \texttt{/the (.*)er they were, the \char`\\ 1er they will be/}, \texttt{\char`\\ 1} should match the same content matched by \texttt{(.*)}.
        \end{example}
\end{description}



\section{Tokenization}

\begin{description}
    \item[Lemma] \marginnote{Lemma}
        Words with the same stem and roughly the same semantic meaning.
        \begin{example}
            \texttt{cat} and \texttt{cats} are the same lemma.
        \end{example}

    \item[Wordform] \marginnote{Wordform}
        Orthographic appearance of a word.
        \begin{example}
            \texttt{cat} and \texttt{cats} do not have the same wordform.
        \end{example}
    
    \item[Vocabulary] \marginnote{Vocabulary}
        Collection of text elements, each indexed by an integer.

        \begin{remark}
            To reduce the size of a vocabulary, words can be reduced to lemmas.
        \end{remark}

    \item[Type / Wordtype] \marginnote{Type / Wordtype}
        Element of a vocabulary (i.e., wordforms in the vocabulary).
    
    \item[Token] \marginnote{Token}
        Instance of a type in a text.

    \item[Genre] \marginnote{Genre}
        Topic of a text corpus (e.g., short social media comments, books, Wikipedia pages, \dots).
\end{description}

\begin{remark}[Herdan's law]
    Given a corpus with $N$ tokens, a vocabulary $V$ over that corpus roughly have size:
    \[ \left\vert V \right\vert = kN^\beta \]
    where the typical values are $10 \leq k \leq 100$ and $0.4 \leq \beta \leq 0.6$.
\end{remark}

\begin{description}
    \item[Stopwords] \marginnote{Stopwords}
        Frequent words that can be dropped.

        \begin{remark}
            If semantics is important, stopwords should be kept. LLMs keep stopwords.
        \end{remark}
\end{description}

\begin{description}
    \item[Rule-based tokenization] \marginnote{Rule-based tokenization} 
        Hand-defined rules for tokenization.
        \begin{remark}
            For speed, simple tokenizers use regex.
        \end{remark}

    \item[Data-driven tokenization] \marginnote{Data-driven tokenization}
        Determine frequent tokens from a large text corpus.
\end{description}


\subsection{Data-driven tokenization}

Tokenization is done by two components:
\begin{descriptionlist}
    \item[Token learner] \marginnote{Token learner}
        Learns a vocabulary from a given corpus (i.e., training).

    \item[Token segmenter] \marginnote{Token segmenter}
        Segments a given input into tokens based on a vocabulary (i.e., inference).
\end{descriptionlist}


\begin{description}
    \item[Byte-pair encoding (BPE)] \marginnote{Byte-pair encoding (BPE)}
        Based on the most frequent $n$-grams.

        \begin{description}
            \item[Token learner]
                Given a training corpus $C$, BPE determines the vocabulary as follows:
                \begin{enumerate}
                    \item Start with a vocabulary $V$ containing all the $1$-grams of $C$ and an empty set of merge rules $M$.
                    \item While the desired size of the vocabulary has not been reached:
                    \begin{enumerate}
                        \item Determine the pair of tokens $t_1 \in V$ and $t_2 \in V$ such that, among all the possible pairs, the $n$-gram $t_1 + t_2 = t_1t_2$ obtained by merging them is the most frequent in the corpus $C$.
                        \item Add $t_1t_2$ to $V$ and the merge rule $t_1+t_2$ to $M$.
                    \end{enumerate}
                \end{enumerate}

                \begin{example}
                    Given the following corpus:
                    \begin{table}[H]
                        \centering
                        \footnotesize
                        \begin{tabular}{cl}
                            \toprule
                            \textbf{Occurrences} & \textbf{Tokens} \\
                            \midrule
                            5 & \texttt{l o w \$} \\
                            2 & \texttt{l o w e r \$} \\
                            6 & \texttt{n e w e s t \$} \\
                            6 & \texttt{w i d e s t \$} \\
                            \bottomrule
                        \end{tabular}
                    \end{table}
                    The initial vocabulary is: $V = \{  \texttt{\$}, \texttt{l}, \texttt{o}, \texttt{w}, \texttt{e}, \texttt{r}, \texttt{n}, \texttt{w}, \texttt{s}, \texttt{t}, \texttt{i}, \texttt{d} \}$.

                    At the first iteration, $\texttt{e} + \texttt{s} = \texttt{es}$ is the most frequent $n$-gram. Corpus and vocabulary are updated as:
                    \begin{table}[H]
                        \centering
                        \footnotesize
                        \begin{tabular}{cl}
                            \toprule
                            \textbf{Occurrences} & \textbf{Tokens} \\
                            \midrule
                            5 & \texttt{l o w \$} \\
                            2 & \texttt{l o w e r \$} \\
                            6 & \texttt{n e w es t \$} \\
                            6 & \texttt{w i d es t \$} \\
                            \bottomrule
                        \end{tabular}
                    \end{table}
                    \vspace{-2em}
                    \[ V = \{  \texttt{\$}, \texttt{l}, \texttt{o}, \texttt{w}, \texttt{e}, \texttt{r}, \texttt{n}, \texttt{w}, \texttt{s}, \texttt{t}, \texttt{i}, \texttt{d} \} \cup \{ \texttt{es} \} \]

                    At the second iteration, $\texttt{es} + \texttt{t} = \texttt{est}$ is the most frequent $n$-gram:
                    \begin{table}[H]
                        \centering
                        \footnotesize
                        \begin{tabular}{cl}
                            \toprule
                            \textbf{Occurrences} & \textbf{Tokens} \\
                            \midrule
                            5 & \texttt{l o w \$} \\
                            2 & \texttt{l o w e r \$} \\
                            6 & \texttt{n e w est \$} \\
                            6 & \texttt{w i d est \$} \\
                            \bottomrule
                        \end{tabular}
                    \end{table}
                    \vspace{-2em}
                    \[ V = \{  \texttt{\$}, \texttt{l}, \texttt{o}, \texttt{w}, \texttt{e}, \texttt{r}, \texttt{n}, \texttt{w}, \texttt{s}, \texttt{t}, \texttt{i}, \texttt{d}, \texttt{es} \} \cup \{ \texttt{est} \} \]

                    And so on\dots
                \end{example}

            \item[Token segmenter]
                Given the vocabulary $V$ and the merge rules $M$, the BPE segmenter does the following:
                \begin{enumerate}
                    \item Split the input into $1$-grams.
                    \item Iteratively scan the input and do the following:
                    \begin{enumerate}
                        \item Apply a merge rule if possible.
                        \item If no merge rules can be applied, lookup the (sub)word in the vocabulary. Tokens out-of-vocabulary are marked with a special unknown token \texttt{[UNK]}.
                    \end{enumerate}
                \end{enumerate}
        \end{description}

    \item[WordPiece] \marginnote{WordPiece}
        Similar to BPE with the addition of merge rules ranking and a special leading/tailing set of characters (usually \texttt{\#\#}) to identify subwords (e.g., \texttt{new\#\#}, \texttt{\#\#est} are possible tokens).

    \item[Unigram] \marginnote{Unigram}
        Starts with a big vocabulary and remove tokens following a loss function.
\end{description}



\section{Normalization}

\begin{description}
    \item[Normalization] \marginnote{Normalization}
        Convert tokens into a standard form.

        \begin{example}
            \texttt{U.S.A.} and \texttt{USA} should be encoded using the same index.
        \end{example}

    \item[Case folding] \marginnote{Case folding}
        Map every token to upper/lower case.

        \begin{remark}
            Depending on the task, casing might be important (e.g., \texttt{US} vs \texttt{us}).
        \end{remark}

    \item[Lemmatization] \marginnote{Lemmatization}
        Reduce inflections and variant forms to their base form.

        \begin{example}
            $\{ \texttt{am}, \texttt{are}, \texttt{is} \} \mapsto \texttt{be}$
        \end{example}

        \begin{remark}
            Accurate lemmatization requires complete morphological parsing.
        \end{remark}

    \item[Stemming] \marginnote{Stemming}
        Reduce terms to their stem.

        \begin{remark}
            Stemming is a simpler approach to lemmatization.
        \end{remark}

        \begin{description}
            \item[Porter stemmer]
                Simple stemmer based on cascading rewrite rules.

                \begin{example}
                    $\texttt{ational} \mapsto \texttt{ate}$, 
                    $\texttt{ing} \mapsto \varepsilon$,
                    $\texttt{sses} \mapsto \texttt{ss}$.
                \end{example}
        \end{description}
\end{description}



\section{Edit distance}

\begin{description}
    \item[Minimum edit distance] \marginnote{Minimum edit distance}
        Minimum number of edit operations (insertions, deletions, and substitutions) needed to transform a string into another one.

        \begin{remark}
            Dynamic programming can be used to efficiently determine the minimum edit distance.
        \end{remark}

    \item[Levenshtein distance] \marginnote{Levenshtein distance}
        Edit distance where:
        \begin{itemize}
            \item Insertions cost $1$;
            \item Deletions cost $1$;
            \item Substitutions cost $2$.
        \end{itemize}

        \begin{example}
            The Levenshtein distance between \texttt{intention} and \texttt{execution} is $8$.
            \begin{table}[H]
                \centering
                \begin{tabular}{cccccccccc}
                    \texttt{I} & \texttt{N} & \texttt{T} & \texttt{E} & \texttt{*} & \texttt{N} & \texttt{T} & \texttt{I} & \texttt{O} & \texttt{N} \\
                    $\vert$ & $\vert$ & $\vert$ & $\vert$ & $\vert$ & $\vert$ & $\vert$ & $\vert$ & $\vert$ & $\vert$ \\
                    \texttt{*} & \texttt{E} & \texttt{X} & \texttt{E} & \texttt{C} & \texttt{U} & \texttt{T} & \texttt{I} & \texttt{O} & \texttt{N} \\
                    $-$ & $\pm$ & $\pm$ &  & $+$ & $\pm$ \\
                    1 & 2 & 2 &  & 1 & 2 \\
                \end{tabular}
            \end{table}
        \end{example}
\end{description}

